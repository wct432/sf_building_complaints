{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements'\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "sys.path.append('../../')\n",
    "from src.pipeline_helpers import get_proportions\n",
    "from src.clean_data import normalize_text\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_addons as tfa\n",
    "from keras.layers import Input, Dense, Embedding, Flatten, Activation, LeakyReLU,Bidirectional, LSTM, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/sf_building_complaints\n"
     ]
    }
   ],
   "source": [
    "print(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = os.getcwd()\n",
    "data_path = os.path.dirname(os.path.dirname(working_dir)) + '/data/'\n",
    "lemmatized_path = data_path + 'lemmatized/'\n",
    "stemmed_path = data_path + 'stemmed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stemmed = np.load(stemmed_path + 'X_stemmed_prepared.npy')\n",
    "y_stemmed = np.load(stemmed_path + 'y_stemmed_prepared.npy')\n",
    "\n",
    "X_lemmatized = np.load(lemmatized_path + 'X_lemmatized_prepared.npy')\n",
    "y_lemmatized = np.load(lemmatized_path + 'y_lemmatized_prepared.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the stemmed tokenizer\n",
    "with open(stemmed_path + 'stemmed_tokenizer_and_weights.pickle', 'rb') as f:\n",
    "    stemmed_data = pickle.load(f)\n",
    "\n",
    "stemmed_class_weights = stemmed_data[0]\n",
    "stemmed_tokenizer = stemmed_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the lemmatized tokenizer\n",
    "with open('lemmatized_tokenizer_and_weights.pickle', 'rb') as f:\n",
    "    lemmatized_data = pickle.load(f)\n",
    "\n",
    "lemmatized_class_weights = lemmatized_data[0]\n",
    "lemmatized_tokenizer = lemmatized_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0: 0.34422726120068387, 1: 5.178850532274937, 2: 0.4602384272550789, 3: 2.638870598563948, 4: 3.2109228949677617, 5: 25.856067588325654}, <keras_preprocessing.text.Tokenizer object at 0x7fcad221ba30>]\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0: 0.3525764366557979, 1: 5.548715624055606, 2: 0.4836829099952055, 3: 1.8841922705908913, 4: 2.8652777777777776, 5: 27.543804380438043}, <keras_preprocessing.text.Tokenizer object at 0x7fcad486f280>]\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.34422726120068387, 1: 5.178850532274937, 2: 0.4602384272550789, 3: 2.638870598563948, 4: 3.2109228949677617, 5: 25.856067588325654}\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.3525764366557979, 1: 5.548715624055606, 2: 0.4836829099952055, 3: 1.8841922705908913, 4: 2.8652777777777776, 5: 27.543804380438043}\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168323, 200)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168323, 6)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  13    5    6 ...    0    0    0]\n",
      " [ 370  212  270 ...    0    0    0]\n",
      " [ 105  208   52 ...    0    0    0]\n",
      " ...\n",
      " [  27  888   27 ...    0    0    0]\n",
      " [ 893 1129   12 ...    0    0    0]\n",
      " [1199  102   25 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " ...\n",
      " [0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168323, 200)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168323, 6)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=.5, random_state = 42, stratify = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check the proportions of each class in the training set and test set to ensure that sklearn's stratify worked as expected. It's crucial to make sure each class is represented the same amount in each subset, so I defined a function in pipeline_helpers to calculate the normalized proportion of each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train proportions: \n",
      " {0: 0.3621322164297702, 1: 0.051909281290380076, 2: 0.006453385613925649, 3: 0.03219266586463485, 4: 0.48418215033640777, 5: 0.0631674315673781}\n",
      "\n",
      "y_test proportions: \n",
      " {0: 0.3621078897338403, 1: 0.05198431558935361, 2: 0.006535171102661597, 3: 0.032200570342205324, 4: 0.4842561787072243, 5: 0.06321292775665399}\n",
      "\n",
      "y_val proportions: \n",
      " {0: 0.3622051921820234, 1: 0.051981227351036656, 2: 0.006475375750014852, 3: 0.03225806451612903, 4: 0.48416800332679855, 5: 0.06320917245886057}\n"
     ]
    }
   ],
   "source": [
    "y_train_proportions = get_proportions(y_train)\n",
    "y_test_proportions = get_proportions(y_test)\n",
    "y_val_proportions = get_proportions(y_val)\n",
    "\n",
    "print(f\"y_train proportions: \\n {y_train_proportions}\\n\")\n",
    "print(f\"y_test proportions: \\n {y_test_proportions}\\n\")\n",
    "print(f\"y_val proportions: \\n {y_val_proportions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WillemCole/miniconda3/envs/sf_building_complaints/lib/python3.9/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4 5] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "# define oversampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy='auto',random_state=42)\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train proportions: \n",
      " {0: 0.16666666666666666, 1: 0.16666922298230008, 2: 0.16666922298230008, 3: 0.16666922298230008, 4: 0.16666922298230008, 5: 0.16666922298230008}\n",
      "\n",
      "y_test proportions: \n",
      " {0: 0.3621078897338403, 1: 0.05198431558935361, 2: 0.006535171102661597, 3: 0.032200570342205324, 4: 0.4842561787072243, 5: 0.06321292775665399}\n",
      "\n",
      "y_val proportions: \n",
      " {0: 0.3622051921820234, 1: 0.051981227351036656, 2: 0.006475375750014852, 3: 0.03225806451612903, 4: 0.48416800332679855, 5: 0.06320917245886057}\n"
     ]
    }
   ],
   "source": [
    "#ensure proportions haven't changed in other sets and are balanced in y_train\n",
    "y_train_proportions = get_proportions(y_train)\n",
    "y_test_proportions = get_proportions(y_test)\n",
    "y_val_proportions = get_proportions(y_val)\n",
    "\n",
    "print(f\"y_train proportions: \\n {y_train_proportions}\\n\")\n",
    "print(f\"y_test proportions: \\n {y_test_proportions}\\n\")\n",
    "print(f\"y_val proportions: \\n {y_val_proportions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391188, 200)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391188, 6)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16833, 6)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391188, 200)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000\n",
    "num_epochs = 20\n",
    "embed_dims = 32\n",
    "\n",
    "metrics = [\n",
    "    tf.keras.metrics.TruePositives(name='tp'),\n",
    "    tf.keras.metrics.FalsePositives(name='fp'),\n",
    "    tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "    tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "    tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='auc'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create callbacks\n",
    "# tqdm callback will display training progress bars \n",
    "model_count = 1\n",
    "model_path = os.path.dirname(working_dir) + '/models/'\n",
    "checkpoint = ModelCheckpoint(filepath=model_path+f'/LSTM/model_{model_count}_best.h5', \n",
    "                             monitor='val_loss',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "tqdm_callback = tfa.callbacks.TQDMProgressBar()\n",
    "\n",
    "callbacks = [checkpoint,early_stopping,tqdm_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 200, 32)           320000    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 64)                16640     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 338,918\n",
      "Trainable params: 338,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#MODEL1\n",
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(num_words, embed_dims, input_length=X_train.shape[1]))\n",
    "model_1.add(Bidirectional(LSTM(embed_dims)))\n",
    "model_1.add(Dense(embed_dims,activation='relu'))\n",
    "model_1.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_1 = tf.keras.optimizers.Adam()\n",
    "model_1.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=opt_1,\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50673b19fa7d42d1a15297f32bf6869f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                                         0/20 ETA: ?s,  ?epochs/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38429c0cdcd943859d35f76854f19bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0/12225                                                               ETA: ?s - "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.72895, saving model to /Users/WillemCole/Desktop/DataScience/Projects/sf_building_complaints/models//LSTM/model_1_best.h5\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a729247b028e4b6a9a69a70c5cd2e29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0/12225                                                               ETA: ?s - "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss did not improve from 0.72895\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7210ec33dd4b0f80237a7c40c90d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0/12225                                                               ETA: ?s - "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss did not improve from 0.72895\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18f4f6b604545689d7b5a3c5532211d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0/12225                                                               ETA: ?s - "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss did not improve from 0.72895\n"
     ]
    }
   ],
   "source": [
    "history_1 = model_1.fit(X_train, y_train, epochs=num_epochs, \n",
    "                    validation_data=(X_val, y_val), verbose=0,\n",
    "                   callbacks=callbacks)\n",
    "model_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 32)           320000    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200, 32)           1056      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200, 32)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                16640     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 341,030\n",
      "Trainable params: 341,030\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#MODEL2\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(num_words, embed_dims, input_length=X_train.shape[1]))\n",
    "model_2.add(Dense(embed_dims,activation='elu'))\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(Bidirectional(LSTM(embed_dims)))\n",
    "model_2.add(Dense(embed_dims,activation='elu'))\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(Dense(embed_dims,activation='elu'))\n",
    "model_2.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_2 = tf.keras.optimizers.Adam()\n",
    "model_2.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=opt_2,\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4dd13fe9e44ff0a8ef7e04869ccc77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                                         0/20 ETA: ?s,  ?epochs/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5950c5e594413dbf48fc94f1847a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0/12225                                                               ETA: ?s - "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss did not improve from 0.72895\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ebe0c70c0444ec85812e0492f4de61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0/12225                                                               ETA: ?s - "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss did not improve from 0.72895\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bedc666737a4c21a44efe77fc1a0e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0/12225                                                               ETA: ?s - "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss did not improve from 0.72895\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313d9dc8056041b488a85cfaa10d7a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0/12225                                                               ETA: ?s - "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss did not improve from 0.72895\n"
     ]
    }
   ],
   "source": [
    "history_2 = model_2.fit(X_train, y_train, epochs=num_epochs, \n",
    "                    validation_data=(X_val, y_val), verbose=0,\n",
    "                   callbacks=callbacks)\n",
    "model_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path + f'/LSTM/model2_history.h5', 'wb') as f:\n",
    "        pickle.dump(history_1.history, f)\n",
    "with open(model_path + f'/LSTM/model2_history.h5', 'wb') as f:\n",
    "        pickle.dump(history_1.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate model_1 on test data\n",
      "526/526 [==============================] - 10s 17ms/step - loss: 0.8991 - tp: 11733.0000 - fp: 4112.0000 - tn: 80048.0000 - fn: 5099.0000 - accuracy: 0.7227 - precision: 0.7405 - recall: 0.6971 - auc: 0.9399\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate model_1 on test data\")\n",
    "results = model_1.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate model_2 on test data\n",
      "526/526 [==============================] - 10s 17ms/step - loss: 0.7976 - tp: 11110.0000 - fp: 4247.0000 - tn: 79913.0000 - fn: 5722.0000 - accuracy: 0.6910 - precision: 0.7234 - recall: 0.6601 - auc: 0.9417\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate model_2 on test data\")\n",
    "results = model_2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.1 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/tensorflow-2.1-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
